main:
  notebooks:
    #- "notebooks/01_data_exploration.ipynb"
    #- "notebooks/02_preprocessing.ipynb"
    #- "notebooks/03_feature_engineering.ipynb"
    - "notebooks/04_sentiment_modeling.ipynb"
    # add more as needed
  timeout: None

exploration:
    raw_csv: "data/raw/IMDB_Dataset.csv"
    save_fig: True
    fig_save_dir: "results/figures/"
    result_file_name: "IMDB_explored.csv"
    result_save_dir: "data/interim/"

preprocessing:
    # -----------------------------
    # PATHS
    # -----------------------------
    paths:
        raw_data: "data/raw/IMDB_Dataset.csv"            # Original Kaggle dataset
        interim_data: "data/interim/IMDB_explored.csv"   # After exploration (word/char stats added)
        cleaned_data: "data/interim/IMDB_cleaned.pkl"    # After full cleaning (format csv, pkl) change the format if not csv
        processed_data: "data/processed/"                # For train/test splits or embeddings
        models: "models/"                                # (Reserved) for saved models later
    
    # -----------------------------
    # TEXT CLEANING SETTINGS
    # -----------------------------
    cleaning:
        lowercase: True                # Convert all text to lowercase for consistency
        strip_html: True               # Remove any HTML tags (e.g., <br />, <p>)
        remove_urls: True              # Remove URLs like "http://..." or "www..."
        remove_emails: True            # Remove email addresses (e.g., user@example.com)
        remove_punctuation: True       # Remove all punctuation characters
        remove_numbers: False          # If true, remove numeric tokens (e.g., 2023, 10)
        remove_control_chars: True     # Remove \n, \t, \r
        remove_emoji: True             # Remove or demojize emoji (e.g., 😍 -> ":smiling_face_with_heart_eyes:")
        normalize_unicode: True        # Normalize Unicode text (e.g., accents → standard ASCII)
        expand_contractions: True      # Expand contractions (e.g., "don't" → "do not")
        replace_whitespace: True       # Collapse multiple spaces/newlines/tabs into a single space
        normalize_elongations: True    # Reduce long character repeats (e.g., "soooo" → "soo")
        remove_stopwords: True         # Remove English stopwords using NLTK
        lemmatize: True                # Lemmatize words (e.g., "running" → "run")
        stem: False                    # (Optional, not used yet) Apply stemming instead of lemmatization
        alpha_only: True               # Keep only alphabetic tokens (discard digits, symbols)
        min_token_length: 2            # Minimum token length to retain (e.g., drop 'a', 'I')
    
    # -----------------------------
    # STOPWORDS CONTROL
    # -----------------------------
    stopwords:
        source: "nltk"                 # Source for stopword list: "nltk" or "custom"
        add_custom: []                 # Add extra words to remove (e.g., ["movie", "film"])
        remove_custom: []              # Words to keep even if they're in the stopword list
    
    # -----------------------------
    # INTERMEDIATE OUTPUT CONTROL
    # -----------------------------
    intermediate:
        save_fig: True                 # Save the figures generated
        fig_dir: "results/figures/"    # Folder to save the figs in
        format: "pkl"                  # Save format for intermediate results
        sample_rows_to_check: 10       # Sample few rows after cleaning to verify transformations

feature_engineering: ## keep the file formats same or everything will break
    # ===== General settings =====
    use_gpu: true                    # Use GPU if available for embeddings
    random_seed: 42                  # Reproducibility
    save_after_each_stage: true      # Save TF-IDF, Word2Vec, etc. after creation
    save_figure: True                # Save Plot or
    format: "pkl"                    # or "npz" for sparse matrices
    cleaned_data: "data/interim/IMDB_cleaned.pkl"
    save_df_path: "data/interim/IMDB_feature_engineered.pkl"
    
    label_encoder:
        save_path: "models/label_encoder.pkl"
        
    count_vectorizer:
        generate_count: False                 # True if you want to generate and save count Vectors
        encoding: "utf-8"                     # Character encoding (e.g., 'latin1' for non-UTF files)
        decode_error: "strict"                # How to handle decode errors: 'strict', 'ignore', or 'replace'
        strip_accents: None                   # 'ascii'/'unicode' removes accents (e.g., “café”→“cafe”)
        ngram_range: (1, 3)                   # Use unigrams only; (1,2) for unigrams + bigrams, etc.
        analyzer: "word"                      # 'word' (default), 'char', or 'char_wb' for character n-grams
        max_df: 0.9                           # Ignore terms in >X% of docs (e.g., 0.9 removes common words)
        min_df: 3                             # Ignore terms in <X docs (e.g., 2 removes rare words)
        max_features: 20000                   # Limit vocab size to top N most frequent tokens
        vocabulary: None                      # Fixed vocabulary dict/list if you already know the terms
        binary: False                         # If True, record presence (1/0) instead of counts
        save_vectorizer_path: "models/count_vectorizer.pkl"
        save_matrix_path: "data/interim/count_vectorized_reviwes.npz"

    
    # ===== TF-IDF Vectorization =====
    tfidf_vectorizer:
        generate_tfidf: False                 # True if you want to generate and save tfidf vectors
        encoding: "utf-8"                     # Character encoding (e.g., 'latin1' for some older corpora)
        decode_error: "strict"                # How to handle decode errors: 'strict', 'ignore', or 'replace'
        strip_accents: None                   # 'ascii'/'unicode' removes accents (“café”→“cafe”)
        analyzer: "word"                      # 'word' (default), 'char', or 'char_wb' (within word boundaries)
        ngram_range: (1, 3)                   # Use unigrams only; (1,2) for unigrams + bigrams, etc.
        max_df: 0.9                           # Ignore terms in >X% of docs (e.g., 0.9 removes very common words)
        min_df: 3                             # Ignore terms in <X docs (e.g., 2 removes very rare words)
        max_features: 20000                   # Keep only the top N features by frequency
        vocabulary: None                      # Fixed vocab list/dict (useful for applying same vocab later)
        binary: False                         # If True, use 1/0 instead of term frequency before TF-IDF scaling
        norm: "l2"                            # Normalize vectors: 'l1', 'l2', or None (no normalization)
        use_idf: True                         # Enable inverse document frequency weighting
        smooth_idf: True                      # Adds 1 to document frequencies to avoid div-by-zero
        sublinear_tf: True                    # If True, use log(1 + tf) for term frequency scaling
        save_vectorizer_path: "models/tfidf_vectorizer.pkl"
        save_matrix_path: "data/interim/tfidf_vectorized_reviwes.npz"
    
    w2v:
        w2v: True
        vector_size: 300
        window: 12
        min_count: 5
        sg: 1
        epochs: 12
        workers: -1
        pooling: "mean" 
        pooling_batch_size: 4096
        model_save_path: "models/w2v.model"
        embeddings_save_path: "data/interim/w2v_review_vecs.npy"
        fig_words: 100
        plot_3d: True
        save_fig_path: "results/figures/w2v_tsne_plot.png"

    fast_text:
        fast_text: True
        vector_size: 300
        window: 12
        min_count: 5
        sg: 1
        epochs: 12
        workers: -1
        pooling: "mean" 
        pooling_batch_size: 4096
        model_save_path: "models/fasttext.model"
        embeddings_save_path: "data/interim/ft_review_vecs.npy"
        fig_words: 100
        plot_3d: True
        save_fig_path: "results/figures/ft_tsne_plot.png"

classical_models: # models in sklearn
    df_path: "data/interim/IMDB_feature_engineered.pkl"
    features: ["cleaned_review_charecter_len","cleaned_review_word_len","positive_tokens_len","negative_tokens_len"]
    target: "sentiment"
    
    count_vectors_path: "data/interim/count_vectorized_reviwes.npz"
    tfidf_vectors_path: "data/interim/tfidf_vectorized_reviwes.npz"
    w2v_vectors_path: "data/interim/w2v_review_vecs.npy"
    fast_text_vectors_path: "data/interim/ft_review_vecs.npy"
    test_size: 0.2
    
    metrics_save_dir: "results/metrics/"
    models_save_dir: "models/"
    
    train_without_cv: True
    train_with_cv: False
    
    embeddings_to_use: ["numerical_cols"] #["numerical_cols", "count", "tfidf", "w2v", "fast_text"] any of these 4 that you generated before
